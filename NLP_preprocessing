#In this code, we conduct tokenization, lemmatization via Spacy so that we may compare the simliarity between texts

import pandas as pd
import numpy as np
from urllib import request
import spacy

#Fetch text for analysis

def download_text(url):
  response = request.urlopen(url)
  text = response.read().decode('utf-8')
  print(f"Type: {type(text)}\nLength: {len(text)}\nFirst 75 characters: {text[:75]}")
  return text

def tokenization(text, max = 793332):
  nlp = spacy.load("en_core_web_sm")
  nlp.max_length = max
  doc = nlp(text)
  return doc

def lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
  text_out = []
  doc = nlp(' '.join(text))
  for token in doc:
    if token.pos_ in allowed_postags:
      text_out.append(token.lemma_)
  return text_out
  
def turn_token_2_df(doc):
  data = []
  for token in doc:
    column_names = ["text", "lemma", "pos", "tag", "alp", "stop"]
    contents = [token.text,
                token.lemma_,
                token.pos_,
                token.tag_,
                token.is_alpha,
                token.is_stop]
    data.append(dict(zip(column_names,contents)))
  df = pd.DataFrame(data)
  return df
  
#The above concludes the main preprocessing steps in using SpaCy, the following is an application of our processed document

Tale_Two_Cities = "https://www.gutenberg.org/files/98/98-0.txt"
TTC_text = download_text(Tale_Two_Cities)
Christmas_Carol = "https://www.gutenberg.org/cache/epub/46/pg46.txt"
CC_text = download_text(CC_text)
Kidnapped_Santa = "https://www.gutenberg.org/cache/epub/519/pg519.txt"
KS_text = download_text(Kidnapped_Santa)

TTC_doc = tokenization(TTC_text)
CC_doc = tokenization(CC_text)
KS_doc = tokenization(KS_text)

print("Tale of Two Cities","<->","A Chirstmas Carol",TTC_doc.similarity(CC_doc))

print("Tale of Two Cities","<->","A Kidnapped Santa Claus",TTC_doc.similarity(KS_doc))

print("A Chirsimas Carol", "<->","A Kidnapped Santa Claus",CC_doc.similarity(KS_doc))
